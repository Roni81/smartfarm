{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roni81/smartfarm/blob/main/prdct_grw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFWnemnbB2Ev",
        "outputId": "7d1ac231-7281-49f7-8562-e27027a94511"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.2-cp310-cp310-manylinux2014_x86_64.whl (98.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.11.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3.post1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Em55lIpcIjK0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from glob import glob\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from itertools import groupby\n",
        "import random\n",
        "\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-PXJesMyIkFu"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_imgs = glob(\"/content/drive/MyDrive/growing2_temp/growing2_temp/images/*.jpg\")\n",
        "train_imgs = sorted(train_imgs)\n",
        "test_imgs = glob(\"/content/drive/MyDrive/growing2_temp/growing2_temp/test/images/*.jpg\")\n",
        "test_imgs = sorted(test_imgs)\n",
        "train_data = glob(\"/content/drive/MyDrive/growing2_temp/growing2_temp/metas/*.csv\")\n",
        "train_data = sorted(train_data)\n",
        "train_label = pd.read_csv(\"/content/drive/MyDrive/growing2_temp/growing2_temp/combined_dataset.csv\")\n",
        "test_data = glob(\"/content/drive/MyDrive/growing2_temp/growing2_temp/test/metas/*.csv\")\n",
        "test_data = sorted(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = \"/content/drive/MyDrive/growing2_temp/growing2_temp\"\n",
        "\n",
        "preprocessing_train_images = main_path + \"/preprocessing_train\"\n",
        "preprocessing_test_images = main_path + \"/preprocessing_test\"\n",
        "\n",
        "if not os.path.exists(preprocessing_train_images):\n",
        "    os.mkdir(preprocessing_train_images)\n",
        "if not os.path.exists(preprocessing_test_images):\n",
        "    os.mkdir(preprocessing_test_images)"
      ],
      "metadata": {
        "id": "VoOrvg-zBtFY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def automatic_brightness_and_contrast(image, clip_hist_percent = 0.025):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "     # Calculate grayscale histogram\n",
        "    hist = cv2.calcHist([gray],[0],None,[256],[0,256])\n",
        "    hist_size = len(hist)\n",
        "\n",
        "    # Calculate cumulative distribution from the histogram\n",
        "    accumulator = []\n",
        "    accumulator.append(float(hist[0]))\n",
        "    for index in range(1, hist_size):\n",
        "        accumulator.append(accumulator[index -1] + float(hist[index]))\n",
        "\n",
        "    # Locate points to clip\n",
        "    maximum = accumulator[-1]\n",
        "    clip_hist_percent *= (maximum/100.0)\n",
        "    clip_hist_percent /= 2.0\n",
        "\n",
        "    # Locate left cut\n",
        "    minimum_gray = 0\n",
        "    while accumulator[minimum_gray] < clip_hist_percent:\n",
        "        minimum_gray += 1\n",
        "\n",
        "    # Locate right cut\n",
        "    maximum_gray = hist_size -1\n",
        "    while accumulator[maximum_gray] >= (maximum - clip_hist_percent):\n",
        "        maximum_gray -= 1\n",
        "\n",
        "    # Calculate alpha and beta values\n",
        "    alpha = 255 / (maximum_gray - minimum_gray)\n",
        "    beta = -minimum_gray * alpha\n",
        "\n",
        "    auto_result = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
        "    return (auto_result)"
      ],
      "metadata": {
        "id": "LbfQIk_nC6oM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_data(dir_in, dir_out):\n",
        "\n",
        "    ratio_lst = []\n",
        "\n",
        "    for i in tqdm(dir_in):\n",
        "        name = i.split(\"\\\\\")[-1] #i.split(\"/\")[-1]\n",
        "        img = cv2.imread(i,cv2.IMREAD_COLOR)\n",
        "        img = cv2.resize(img, (1000,750))\n",
        "        brightscale = automatic_brightness_and_contrast(img)\n",
        "        imgcopy = brightscale.copy()\n",
        "        hsvimage = cv2.cvtColor(brightscale,cv2.COLOR_BGR2HSV)\n",
        "        lower = np.array([22,40,0])\n",
        "        upper = np.array([85,255,245])\n",
        "        mask = cv2.inRange(hsvimage, lower, upper)\n",
        "        number_of_white_pix = np.sum(mask == 255)\n",
        "        number_of_black_pix = np.sum(mask == 0)\n",
        "        ratio = number_of_white_pix / (number_of_white_pix + number_of_black_pix)\n",
        "        ratio_lst.append(ratio)\n",
        "        result = cv2.bitwise_and(imgcopy, imgcopy, mask = mask)\n",
        "        cv2.imwrite(os.path.join(dir_out, name), result)\n",
        "\n",
        "    return ratio_lst\n"
      ],
      "metadata": {
        "id": "uZbcal0rDxQQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratio_train = get_image_data(train_imgs, preprocessing_train_images)\n",
        "ratio_test = get_image_data(test_imgs, preprocessing_test_images)\n",
        "\n",
        "processed_train_imgs = glob(main_path + \"/preprocessing_train/*.jpg\")\n",
        "processed_train_imgs = sorted(processed_train_imgs)\n",
        "\n",
        "processed_test_imgs = glob(main_path + \"/preprocessing_test/*.jpg\")\n",
        "processed_test_imgs = sorted(processed_test_imgs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8InAJJ5NFlcl",
        "outputId": "acf3b22e-f203-4ed2-8ba2-5daa6f63a240"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1592/1592 [22:54<00:00,  1.16it/s]\n",
            "100%|██████████| 246/246 [04:04<00:00,  1.01it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = []\n",
        "\n",
        "for i in tqdm(train_data):\n",
        "    name = i.split(\"\\\\\")[-1].split(\".\")[0]\n",
        "    df = pd.read_csv(i)\n",
        "    df = df.drop('시간', axis=1)\n",
        "    case = name.split(\"_\")[0]\n",
        "    label = pd.read_csv(\"/content/drive/MyDrive/growing2_temp/growing2_temp/combined_dataset.csv\")\n",
        "\n",
        "    # 이미지 이름이 일치하는 경우에만 무게 정보를 가져옴\n",
        "    if name in label.img_name.values:\n",
        "        leaf_weight = label[label.img_name == name].leaf_weight.values[0]\n",
        "        df[\"무게\"] = leaf_weight\n",
        "        df[\"최근분무량\"] = df[\"최근분무량\"].fillna(method='bfill', limit=1)\n",
        "        df[\"최근분무량\"] = df[\"최근분무량\"].fillna(method='ffill', limit=1)\n",
        "        df = df.interpolate()\n",
        "        water = df['최근분무량'].round(2).tolist()\n",
        "        if np.mean(water) > 1000:\n",
        "            nums = [list(v) for k, v in groupby(water, key=lambda x: x != 0) if k != 0]\n",
        "            if len(nums) == 2:\n",
        "                cumulative = nums[0][-1] - nums[0][0] + nums[1][-1]\n",
        "            else:\n",
        "                cumulative = nums[0][-1] - nums[0][0]\n",
        "        elif 1000 > np.mean(water) > 0:\n",
        "            nums = [key for key, _group in groupby(water)]\n",
        "            cumulative = sum(nums[1:])\n",
        "        else:\n",
        "            cumulative = 0\n",
        "\n",
        "        # df DataFrame이 비어 있지 않은 경우에만 추가\n",
        "        if not df.empty:\n",
        "            df = df.mean().to_frame().T\n",
        "            df[\"이미지\"] = name\n",
        "            df['최근분무량'] = cumulative\n",
        "            train_df.append(df)\n",
        "\n",
        "# train_df에 정보가 있는 경우에만 pd.concat 호출\n",
        "if train_df:\n",
        "    train_df = pd.concat(train_df, ignore_index=True)\n",
        "    train_df['비율'] = ratio_train\n",
        "    train_df.head()\n",
        "else:\n",
        "    print(\"No objects to concatenate. train_df is empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfzZaUFvIWot",
        "outputId": "9c825290-c38f-44fc-a484-da991f21d1f6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1592/1592 [00:17<00:00, 92.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No objects to concatenate. train_df is empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = []\n",
        "for i in tqdm(test_data):\n",
        "    name = i.split(\"\\\\\")[-1].split(\".\")[0]  # i.split(\"/\")[-1].split(\".\")[0]\n",
        "    df = pd.read_csv(i)\n",
        "    df = df.drop('시간', axis = 1)\n",
        "    df[\"최근분무량\"] = df[\"최근분무량\"].fillna(method='bfill', limit=1)\n",
        "    df[\"최근분무량\"] = df[\"최근분무량\"].fillna(method='ffill', limit=1)\n",
        "    df = df.interpolate()\n",
        "    water = df['최근분무량'].round(2).tolist()\n",
        "    if np.mean(water) > 1000:\n",
        "        nums = [list(v) for k,v in groupby(water, key = lambda x: x != 0) if k != 0]\n",
        "        if len(nums) == 2:\n",
        "            cumulative = nums[0][-1] - nums[0][0] + nums[1][-1]\n",
        "        else:\n",
        "            cumulative = nums[0][-1] - nums[0][0]\n",
        "\n",
        "    elif 1000 > np.mean(water) > 0:\n",
        "        nums = [key for key, _group in groupby(water)]\n",
        "        cumulative = sum(nums[1:])\n",
        "    else:\n",
        "        cumulative = 0\n",
        "\n",
        "    df = df.mean()\n",
        "    df = df.to_frame().T\n",
        "    df[\"이미지\"] = name\n",
        "    df['최근분무량'] = cumulative\n",
        "\n",
        "    test_df.append(df)\n",
        "\n",
        "\n",
        "if test_df:\n",
        "    test_df = pd.concat(test_df, ignore_index=True)\n",
        "    test_df['비율'] = ratio_train\n",
        "    test_df.head()\n",
        "else:\n",
        "    print(\"No objects to concatenate. train_df is empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4V8Mn6xL9Ht",
        "outputId": "c10ef6f3-091f-4745-dac9-ca22243ffc9b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No objects to concatenate. train_df is empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(train_df, x = '무게', y= '비율',\n",
        "                 hover_name=\"이미지\", trendline=\"ols\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "PjcGHQWYMC_t",
        "outputId": "39d94146-d23d-4b3c-86cc-8f2d1ea9b1e4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-4c9c4684cca0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m fig = px.scatter(train_df, x = '무게', y= '비율',\n\u001b[0m\u001b[1;32m      2\u001b[0m                  hover_name=\"이미지\", trendline=\"ols\")\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/express/_chart_types.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(data_frame, x, y, color, symbol, size, hover_name, hover_data, custom_data, text, facet_row, facet_col, facet_col_wrap, facet_row_spacing, facet_col_spacing, error_x, error_x_minus, error_y, error_y_minus, animation_frame, animation_group, category_orders, labels, orientation, color_discrete_sequence, color_discrete_map, color_continuous_scale, range_color, color_continuous_midpoint, symbol_sequence, symbol_map, opacity, size_max, marginal_x, marginal_y, trendline, trendline_options, trendline_color_override, trendline_scope, log_x, log_y, range_x, range_y, render_mode, title, template, width, height)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0mmark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0mD\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \"\"\"\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmake_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36mmake_figure\u001b[0;34m(args, constructor, trace_patch, layout_patch)\u001b[0m\n\u001b[1;32m   2001\u001b[0m     \u001b[0mapply_default_cascade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2003\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2004\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconstructor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreemap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSunburst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIcicle\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2005\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataframe_hierarchy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36mbuild_dataframe\u001b[0;34m(args, constructor)\u001b[0m\n\u001b[1;32m   1410\u001b[0m     \u001b[0;31m# now that things have been prepped, we do the systematic rewriting of `args`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m     df_output, wide_id_vars = process_args_into_dataframe(\n\u001b[0m\u001b[1;32m   1413\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwide_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/plotly/express/_core.py\u001b[0m in \u001b[0;36mprocess_args_into_dataframe\u001b[0;34m(args, wide_mode, var_name, value_name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0margument\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"index\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"\\n To use the index, pass it in directly as `df.index`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1208\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1209\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     raise ValueError(\n",
            "\u001b[0;31mValueError\u001b[0m: Value of 'x' is not the name of a column in 'data_frame'. Expected one of [] but received: 무게"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_outliers = ['CASE05_21','CASE05_22','CASE05_23', 'CASE07_07', 'CASE07_08', 'CASE16_03', 'CASE23_01', 'CASE23_02',\n",
        "'CASE23_03', 'CASE23_04', 'CASE23_05', 'CASE23_06', 'CASE23_07', 'CASE23_08', 'CASE23_09', 'CASE45_16', 'CASE45_17',\n",
        "'CASE72_06',  'CASE73_10', 'CASE59_01','CASE59_02','CASE59_03','CASE59_04','CASE59_05','CASE59_06',\n",
        "'CASE59_07','CASE59_08','CASE59_09','CASE59_10','CASE59_11','CASE59_12','CASE59_13','CASE59_14','CASE59_15','CASE59_16','CASE59_17','CASE59_18',\n",
        "'CASE59_19','CASE59_20','CASE59_21','CASE59_22','CASE59_23','CASE59_24','CASE59_25','CASE59_26','CASE59_27','CASE59_28','CASE59_29','CASE59_30',\n",
        "'CASE59_31','CASE59_32', 'CASE59_33']\n",
        "\n",
        "train_df_image = train_df[~train_df['이미지'].isin(image_outliers)]\n",
        "train_imgs_removed = [ x for x in processed_train_imgs if x.split(\".\")[1].split(\"\\\\\")[1] not in image_outliers]  # x.split(\".\")[1].split(\"/\")[3]"
      ],
      "metadata": {
        "id": "TOPZXtcGXG5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"The code will run on GPU.\")\n",
        "else:\n",
        "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")"
      ],
      "metadata": {
        "id": "7-13HwngXNZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CFG = {\n",
        "    'IMG_SIZE':128,\n",
        "    'EPOCHS':80,\n",
        "    'LEARNING_RATE':1e-3,\n",
        "    'BATCH_SIZE':32,\n",
        "    'SEED':42\n",
        "}"
      ],
      "metadata": {
        "id": "E82W6ZXzXRh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED']) # Seed 고정"
      ],
      "metadata": {
        "id": "nFIiQMEcXTq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = int(len(train_imgs_removed)*0.8)\n",
        "weight = train_df_image['무게'].round(3).tolist()\n",
        "\n",
        "train_img_path = train_imgs_removed[:train_len]\n",
        "train_label = weight[:train_len]\n",
        "\n",
        "vali_img_path = train_imgs_removed[train_len:]\n",
        "vali_label = weight[train_len:]"
      ],
      "metadata": {
        "id": "xRb72r_pXXof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_path_list, label_list, train_mode=True, transforms=None):\n",
        "        self.transforms = transforms\n",
        "        self.train_mode = train_mode\n",
        "        self.img_path_list = img_path_list\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def __getitem__(self, index): # Use index when calling images\n",
        "        img_path = self.img_path_list[index]\n",
        "        # Get image data\n",
        "        image = cv2.imread(img_path)\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        if self.train_mode:\n",
        "            label = self.label_list[index]\n",
        "            return image, label\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "    def __len__(self): # Returns number of training data\n",
        "        return len(self.img_path_list)"
      ],
      "metadata": {
        "id": "P1kha2HCXaKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
        "                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "                    ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Resize((CFG['IMG_SIZE'], CFG['IMG_SIZE'])),\n",
        "                    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "                    ])"
      ],
      "metadata": {
        "id": "N-1zA6HuXdJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_img_path, train_label, train_mode=True, transforms=train_transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
        "\n",
        "vali_dataset = CustomDataset(vali_img_path, vali_label, train_mode=True, transforms=test_transform)\n",
        "vali_loader = DataLoader(vali_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "zd--nPisXfXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNRegressor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNRegressor, self).__init__()\n",
        "        self.layer1 = torch.nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.layer2 = torch.nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.layer3 = torch.nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.layer4 = torch.nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.regressor = nn.Linear(3136,1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Simple CNN Model (Batch, 3, 128, 128 -> Batch, 64, 7, 7)\n",
        "        # (Batch, 3, 128, 128)\n",
        "        x = self.layer1(x)\n",
        "        # (Batch, 8, 64, 64)\n",
        "        x = self.layer2(x)\n",
        "        # (Batch, 16, 32, 32)\n",
        "        x = self.layer3(x)\n",
        "        # (Batch, 32, 16, 16)\n",
        "        x = self.layer4(x)\n",
        "        # (Batch, 64, 7, 7) -> Flatten (Batch, 64*7*7(=3136))\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        # Regressor (Batch, 3136) -> (Batch, 1)\n",
        "        out = self.regressor(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "SNdsDnH3XjSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, train_loader, vali_loader, scheduler, device):\n",
        "    model.to(device)\n",
        "\n",
        "    # Loss Function\n",
        "    criterion = nn.L1Loss().to(device)\n",
        "    best_mae = 9999\n",
        "\n",
        "    for epoch in range(1,CFG[\"EPOCHS\"]+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        for img, label in tqdm(iter(train_loader)):\n",
        "            img, label = img.float().to(device), label.float().to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Data -> Model -> Output\n",
        "            logit = model(img)\n",
        "            # Calc loss\n",
        "            loss = criterion(logit.squeeze(1), label)\n",
        "\n",
        "            # backpropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Evaluation Validation set\n",
        "        vali_mae = validation(model, vali_loader, criterion, device)\n",
        "\n",
        "        print(f'Epoch [{epoch}] Train MAE : [{np.mean(train_loss):.5f}] Validation MAE : [{vali_mae:.5f}]\\n')\n",
        "\n",
        "        # Model Saved\n",
        "        if best_mae > vali_mae:\n",
        "            best_mae = vali_mae\n",
        "            torch.save(model.state_dict(), './best_model.pth')\n",
        "            print('Model Saved.')"
      ],
      "metadata": {
        "id": "AuEjH16cXmRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, vali_loader, criterion, device):\n",
        "    model.eval() # Evaluation\n",
        "    vali_loss = []\n",
        "    with torch.no_grad():\n",
        "        for img, label in tqdm(iter(vali_loader)):\n",
        "            img, label = img.float().to(device), label.float().to(device)\n",
        "\n",
        "            logit = model(img)\n",
        "            loss = criterion(logit.squeeze(1), label)\n",
        "\n",
        "            vali_loss.append(loss.item())\n",
        "\n",
        "    vali_mae_loss = np.mean(vali_loss)\n",
        "    return vali_mae_loss"
      ],
      "metadata": {
        "id": "k9bSLAWAXpBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNNmodel = CNNRegressor().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(params = CNNmodel.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
        "scheduler = None\n",
        "\n",
        "train(CNNmodel, optimizer, train_loader, vali_loader, scheduler, device)"
      ],
      "metadata": {
        "id": "7EMSPQfqXrkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_loader, device):\n",
        "    model.eval()\n",
        "    model_pred = []\n",
        "    with torch.no_grad():\n",
        "        for img in tqdm(iter(test_loader)):\n",
        "            img = img.float().to(device)\n",
        "\n",
        "            pred_logit = model(img)\n",
        "            pred_logit = pred_logit.squeeze(1).detach().cpu()\n",
        "\n",
        "            model_pred.extend(pred_logit.tolist())\n",
        "    return model_pred"
      ],
      "metadata": {
        "id": "iMDVy7BBXvXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = CustomDataset(processed_test_imgs, None, train_mode=False, transforms=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)\n",
        "\n",
        "# Validation Score가 가장 뛰어난 모델을 불러옵니다.\n",
        "checkpoint = torch.load('./best_model.pth')\n",
        "CNNmodel = CNNRegressor().to(device)\n",
        "CNNmodel.load_state_dict(checkpoint)\n",
        "\n",
        "# Inference\n",
        "preds = predict(CNNmodel, test_loader, device)"
      ],
      "metadata": {
        "id": "Fu52kZcXXyja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('./open/sample_submission.csv')\n",
        "submission['leaf_weight'] = preds\n",
        "submission.to_csv('./CNNsubmit.csv', index=False)"
      ],
      "metadata": {
        "id": "w-45VzEUX1pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://dacon.io/competitions/official/235897/codeshare/5017?page=1&dtype=recent"
      ],
      "metadata": {
        "id": "UKgHl40NYJDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metadata EDA\n",
        "각 환경 변수 시각화하면서 이상값 판단하기\n",
        "CASE01, CASE02 경우 EC 관측치, 외부온도 값이 다른 케이스에 비해 매우 다르므로 메타데이터에 제외하기\n",
        "음수 값이 나오는 최근분무량 (일간누적분무량) 제외하기 (일부 CASE04)\n",
        "CO2관측지가 0인 케이스는 누락 데이터로 판단하여 메타데이터에 제외하기"
      ],
      "metadata": {
        "id": "5rliYzvRYCNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "firstfeats = ['내부온도관측치', '외부온도관측치', '내부습도관측치', '외부습도관측치', 'CO2관측치', 'EC관측치','최근분무량']\n",
        "\n",
        "secondfeats = ['냉방온도', '냉방부하','난방온도', '난방부하', '비율']\n",
        "\n",
        "thirdfeats = ['화이트 LED동작강도', '레드 LED동작강도', '블루 LED동작강도', '총추정광량', '백색광추정광량', '적색광추정광량', '청색광추정광량']\n",
        "\n",
        "for feat in firstfeats:\n",
        "    fig = make_subplots(rows=1, cols=2)\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x = train_df[feat].index, y =  train_df[feat], text=train_df[\"이미지\"]),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x = test_df[feat].index, y =  test_df[feat], text=test_df[\"이미지\"]),\n",
        "        row=1, col=2\n",
        "    )\n",
        "    fig.update_layout(showlegend=False, title_text=feat)\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "iWOmTzPfYL2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_outliers = ['CASE01_01','CASE01_02','CASE01_03','CASE01_04','CASE01_05','CASE01_06','CASE01_07',\n",
        "'CASE01_08','CASE01_09','CASE02_01','CASE02_02','CASE02_03','CASE02_04','CASE02_05','CASE02_06','CASE02_07',\n",
        "'CASE02_08','CASE02_09','CASE02_10','CASE02_11']\n",
        "\n",
        "train_df_meta = train_df_image[~train_df_image['이미지'].isin(meta_outliers)]\n",
        "\n",
        "train_df_meta = train_df_meta[train_df_meta['CO2관측치'] > 0]\n",
        "train_df_meta = train_df_meta[train_df_meta['최근분무량'] >= 0]"
      ],
      "metadata": {
        "id": "LaYbjneMYUmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = train_df_meta.corr()\n",
        "\n",
        "corr.style.background_gradient(cmap='coolwarm')"
      ],
      "metadata": {
        "id": "CTilF2dkYWSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter(train_df_meta, x = train_df_meta['총추정광량'],\n",
        "                 y= train_df_meta['백색광추정광량'] + train_df_meta['적색광추정광량']+ train_df_meta['청색광추정광량'],\n",
        "                 trendline=\"ols\")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "QylCap5kYc4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['내부온도관측치', '외부온도관측치', '내부습도관측치', '외부습도관측치', 'CO2관측치', 'EC관측치',\n",
        "         '최근분무량', '냉방온도', '냉방부하',\n",
        "         '난방온도', '난방부하', '백색광추정광량', '적색광추정광량', '청색광추정광량', '비율']\n",
        "\n",
        "train_col = train_df_meta[features]\n",
        "\n",
        "test_col = test_df[features]\n",
        "\n",
        "train_target = train_df_meta[\"무게\"]\n",
        "\n",
        "train_x, val_x, train_y, val_y = train_test_split(train_col, train_target, test_size=0.2, random_state=32)"
      ],
      "metadata": {
        "id": "3dtvhYqEYfM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CatBoost Fit\n",
        "50번 이상 validation loss 개선 없을 경우 조기종료"
      ],
      "metadata": {
        "id": "6fsdegUcYhHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATmodel = CatBoostRegressor(verbose=50,\n",
        "                             n_estimators=10000,\n",
        "                             eval_metric='MAE',\n",
        "                             early_stopping_rounds=50)\n",
        "CATmodel.fit(train_x, train_y, eval_set=[(val_x, val_y)],\n",
        "                   use_best_model=True)\n",
        "\n",
        "val_pred = CATmodel.predict(val_x)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(np.array(val_pred),label = \"pred\")\n",
        "plt.plot(np.array(val_y),label = \"true\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "train_score = CATmodel.score(train_x, train_y) # train (learn) score\n",
        "\n",
        "val_score = CATmodel.score(val_x, val_y) # val (test) score"
      ],
      "metadata": {
        "id": "Fw82S6DHYknT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CATresult = CATmodel.predict(test_col)\n",
        "\n",
        "submission = pd.read_csv('./open/sample_submission.csv')\n",
        "submission['leaf_weight'] = CATresult\n",
        "submission.to_csv('./CATsubmit.csv', index=False)"
      ],
      "metadata": {
        "id": "yR88wSEGYumv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANN Fit\n",
        "싸이킷런(스케일러)로 메타데이터 스케일 조정\n",
        "50번 이상 validation loss 개선 없을 경우 조기종료"
      ],
      "metadata": {
        "id": "EfLQCM0PYzLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_datasets(x_train, x_test):\n",
        "  \"\"\"\n",
        "  Standard Scale test and train data\n",
        "  Z - Score normalization\n",
        "  \"\"\"\n",
        "  standard_scaler = StandardScaler()\n",
        "  x_train_scaled = pd.DataFrame(\n",
        "      standard_scaler.fit_transform(x_train),\n",
        "      columns=x_train.columns\n",
        "  )\n",
        "  x_test_scaled = pd.DataFrame(\n",
        "      standard_scaler.transform(x_test),\n",
        "      columns = x_test.columns\n",
        "  )\n",
        "  return x_train_scaled, x_test_scaled\n",
        "\n",
        "train_scaled, test_scaled = scale_datasets(train_col, test_col)\n",
        "\n",
        "train_x_scale, val_x_scale, train_y_scale, val_y_scale = train_test_split(train_scaled,\n",
        "                                                                          train_target,\n",
        "                                                                          test_size=0.2,\n",
        "                                                                          random_state=32)"
      ],
      "metadata": {
        "id": "7gge8rgcY3JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating model using the Sequential in tensorflow\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "def build_model_using_sequential():\n",
        "  model = Sequential([\n",
        "    Dense(100, kernel_initializer='normal', activation='relu'),\n",
        "    Dense(50, kernel_initializer='normal', activation='relu'),\n",
        "    Dense(25, kernel_initializer='normal', activation='relu'),\n",
        "    Dense(1, kernel_initializer='normal', activation='linear')\n",
        "  ])\n",
        "  return model\n",
        "# build the model\n",
        "ANNmodel = build_model_using_sequential()\n",
        "\n",
        "# loss function\n",
        "mae = MeanAbsoluteError()\n",
        "ANNmodel.compile(\n",
        "    loss=mae,\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    metrics=[mae]\n",
        ")\n",
        "\n",
        "early_stopping_monitor = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    min_delta=0,\n",
        "    patience=50,\n",
        "    verbose=1,\n",
        "    mode='auto',\n",
        "    baseline=None,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# train the model\n",
        "history = ANNmodel.fit(\n",
        "    train_x_scale,\n",
        "    train_y_scale,\n",
        "    epochs=1000,\n",
        "    batch_size=32,\n",
        "    validation_data=(val_x_scale, val_y_scale),\n",
        "    callbacks=[early_stopping_monitor],\n",
        "    verbose= 2\n",
        ")"
      ],
      "metadata": {
        "id": "PIiZDvQRZCRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_pred = ANNmodel.predict(val_x_scale)\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.plot(np.array(val_pred),label = \"pred\")\n",
        "plt.plot(np.array(val_y_scale),label = \"true\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C8b0TUSLZHmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANNresult = ANNmodel.predict(test_scaled)\n",
        "\n",
        "submission = pd.read_csv('./open/sample_submission.csv')\n",
        "submission['leaf_weight'] = ANNresult\n",
        "submission.to_csv('./ANNsubmit.csv', index=False)"
      ],
      "metadata": {
        "id": "up5kDchNZIfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble\n",
        "각 모델 결과 비교하면서 public score가 더 좋을수록 가중치 높게 적용\n",
        "CNN (0.152884733) * 0.65 + CatBoost (0.2221573479) * 0.25 + Ann (0.2557698871) * 0.1"
      ],
      "metadata": {
        "id": "AbbFd5C5ZMX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CNN = pd.read_csv('./CNNsubmit.csv')\n",
        "CAT = pd.read_csv('./CATsubmit.csv')\n",
        "ANN = pd.read_csv('./ANNsubmit.csv')\n",
        "\n",
        "submission_final = pd.read_csv('./open/sample_submission.csv')\n",
        "submission_final['leaf_weight'] = (CNN['leaf_weight'] * 0.65 + CAT['leaf_weight'] * 0.25 + ANN['leaf_weight'] * 0.1)\n",
        "submission_final.to_csv('ENSEMBLEsubmit.csv', index=False)"
      ],
      "metadata": {
        "id": "xF0FZGQhZQ0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1tuc8qb3q5ZghpDzdArMdMEu9w92VvLGe",
      "authorship_tag": "ABX9TyP03H/D2AwshN4W/1evuuNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}